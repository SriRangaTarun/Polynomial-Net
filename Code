" This model predicts the degree of a polynomial relationship between two scalar continuous variables given empirical data.
  Predicting the degree could take a maximum of 60 seconds on a 3.1 GHz Intel i5 core on an iMac. The times are almost always 
  close to or below 50 seconds. It could even take less than 40 seconds for some low degree tasks. Note that the run times are 
  extremely unpredictable. This model needs at least 15 to 20 datapoints to give correct predictions."
  
  
import numpy as np
import tensorflow as tf
from keras import backend as K

sess = tf.InteractiveSession()
K.set_session(sess)



def preprocessData(X, Y):
    # preprocesses data
    
    import numpy as np
    
    n_samples = len(X)
    
    if min(X) < 0:          
        X = np.array(X) - [min(np.array(X))]*len(X)
    
    if max(Y) < 0:
        Y = [-Y[i] for i in range(0, len(Y))]
    
    if (Y[0] > Y[n_samples // 2] and Y[n_samples // 2] > Y[n_samples - 1]) or (Y[0] < Y[n_samples // 2] and Y[n_samples - 1] < Y[n_samples // 2]):
        Y = [-Y[i] for i in range(0, len(Y))]
        
    X = list(X)
    Y = list(Y)
    
    threshold = 100
    
    X1 = X
    Y1 = Y
    
    if len(X) < threshold:
        while len(X) <= threshold and len(Y) <= threshold:
            X = X + X1
            Y = Y + Y1 
            
    X = X + [-X[i] for i in range(0, len(X))]
    Y = Y + [-Y[i] for i in range(0, len(Y))]
            
 # All the above three if cases manipulate the input data such that the model can find the degree more convenietly 

    X = np.array(X)
    Y = np.array(Y)
    
    X = np.reshape(X, (len(X), 1))
    Y = np.reshape(Y, (len(Y), 1))
    
    return X, Y
    
    
def buildModel():                                                  # builds polynomial net model with layers with exponential
                                                                   # output
    import keras
    import tensorflow as tf
    
    from keras.models import Model
    from keras.layers import Input, Dense, Lambda, multiply, add
    
    a = tf.Variable(1.0)
    b = tf.Variable(1.0)
    n = tf.Variable(1.0)
    
    x = Input(shape=(1,))
    
    j1 = 1.0
    j2 = 10.0
                                 # Hyperparameters that have been set by continuous trail and error
    k1 = 1.0
    k2 = 10.0
    
    dense_a = Dense(2, kernel_initializer='glorot_normal', bias_initializer='glorot_normal')(x)
    powered_dense1 = Lambda(lambda x: tf.abs(-j1*tf.sigmoid(j2*a)*x**2 - k1*tf.sigmoid(k2*b))**(n/8.0))(Dense(1, kernel_initializer='glorot_normal', bias_initializer='glorot_normal')(dense_a))
    model1 = Model(x, powered_dense1)
    model1.layers[-1].trainable_weights.extend([a, b, n])
    
    dense_b = Dense(2, kernel_initializer='glorot_normal', bias_initializer='glorot_normal')(x)
    powered_dense2 = Lambda(lambda x: tf.abs(-j1*tf.sigmoid(j2*a)*x**2 - k1*tf.sigmoid(k2*b))**(n/8.0))(Dense(1, kernel_initializer='glorot_normal', bias_initializer='glorot_normal')(dense_b))
    model2 = Model(x, powered_dense2)
    model2.layers[-1].trainable_weights.extend([a, b, n])
    
    

                                                              # "n" is the parameter representing the degree
                                                              
                                                              # We initialize all weights and biases in the dense layers using
                                                              # the Glorot or Xavier Normal Initializer
                                                     
                                                     
    dense_c = Dense(2, kernel_initializer='glorot_normal', bias_initializer='glorot_normal')(x)
    powered_dense3 = Lambda(lambda x: tf.abs(j1*tf.sigmoid(j2*a)*x**2 + k1*tf.sigmoid(k2*b))**(n/8.0))(Dense(1, kernel_initializer='glorot_normal', bias_initializer='glorot_normal')(dense_c))
    model3 = Model(x, powered_dense3)
    model3.layers[-1].trainable_weights.extend([a, b, n])

    dense_d = Dense(2, kernel_initializer='glorot_normal', bias_initializer='glorot_normal')(x)
    powered_dense4 = Lambda(lambda x: tf.abs(j1*tf.sigmoid(j2*a)*x**2 + k1*tf.sigmoid(k2*b))**(n/8.0))(Dense(1, kernel_initializer='glorot_normal', bias_initializer='glorot_normal')(dense_d))
    model4 = Model(x, powered_dense4)
    model4.layers[-1].trainable_weights.extend([a, b, n])
    
    exponentialLayers = multiply([model1.output, model2.output, model3.output, model4.output])
    
    model = Model(x, exponentialLayers)
    model.layers[-1].trainable_weights.extend([a, b, n])
    
    return model, n
    
    
    
    
def trainModel(X, Y, model):                           # fits model to data
    model.fit(X, Y, epochs=1, verbose=0)
    
    
    
def predictDegree(X, Y):  
    import tensorflow as tf
    import numpy as np                                # Predicts degree of polynomial relationship between X and Y
    from math import isnan
    
    X, Y = preprocessData(X, Y)
    model, n = buildModel()
    
    from keras.optimizers import RMSprop
    model.compile(loss='mae', optimizer=RMSprop(lr=0.01))
    
    tf.global_variables_initializer().run()
    
    constant = 350000
    
    X_list = np.reshape(X, (len(X)))
    epochs = constant//(len(X_list))                  # Number of iterations decreases with increase in size of data
                                                      # You can experiment with different values of "constant"
    
    for i in range(0, epochs):
        trainModel(X, Y, model)             
        i = i + 1
        
    if sess.run(n) < 0:
        predictDegree(X, Y)
        
    return np.int32(np.round(sess.run(n)))    # return rounded degree      
    
 ***************************************************************************************************************************
           
                                             EXAMPLE OF USING THIS MODEL:
                                            
 Check out this polynomial regression dataset here: http://users.stat.ufl.edu/~winner/data/heatcapacity.dat and the data
 description here : http://users.stat.ufl.edu/~winner/data/heatcapacity.txt. I will use model to find the degree of the
 polynomial relationship between the two variables involved here: The heat capacities and temperatures of solid Hydrogen
 Bromide in 18 experimental runs. We take the heat capacity as the X variable and the temperature as the Y variable (the
 units of measure are given in the data description).

X = [10.79,  
     10.80,  
     10.86,  
     10.93,  
     10.99,  
     10.96,  
     10.98,
     11.03,  
     11.08,  
     11.10,  
     11.19,  
     11.25,  
     11.40,  
     11.61,  
     11.69,  
     11.91,  
     12.07,  
     12.32]

Y = [118.99,
     120.76,
     122.71,
     125.48,
     127.31,
     130.06,
     132.41,
     135.89,
     139.02,
     140.25,
     145.61,
     153.45,
     158.03,
     162.72,
     167.67,
     172.86,
     177.52,
     182.09]
     
import time

startTime = time.time()
print("Degree of polynomial relationship : " + str(predictDegree(X, Y)) + " (if there is a polynomial relationship)")
endTime = time.time()
print("Time taken : " + str(endTime - startTime) + " s")

# In the above code, we predict the degree and measure the time the whole process takes.
                           

     
import numpy as np
degree = predictDegree(X, Y)
best_fit = np.polyfit(X, Y, degree)    # Coefficients of the best fitting polynomial of the "correct" degree

YHat = np.multiply([best_fit[0]]*len(X), np.multiply(X, X)) + np.multiply([best_fit[1]]*len(X), X) + [best_fit[2]]*len(X)
# The predicted degree is 2 in this case and therefore the best_fit array has 3 elements for 3 coefficients

import matplotlib.pyplot as plt
plt.plot(X, Y, 'r.')
plt.plot(X, YHat, 'k')
plt.show()

# Above, we plot and visualize the best fitting curve of the required degree alongside the actual data. The data does seem to 
  follow a quadratic curve. The model outputs the correct degree (which is 2). The whole degree prediction process takes 
  approximately 40.28 seconds on a 3.1 GHz Intel i5 core on an iMac.
