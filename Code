" This model predicts the degree of a polynomial relationship between two scalar continuous variables given empirical data.
  Predicting the degree could take a maximum of 60 seconds on a 3.1 GHz Intel i5 core on an iMac. The times are almost always 
  close to or below 40 seconds. It could even take less than 20 seconds for some low degree tasks. Note that the run times are 
  extremely unpredictable."
  
  
import numpy as np
import tensorflow as tf
from keras import backend as K

sess = tf.InteractiveSession()
K.set_session(sess)



def preprocessData(X, Y):

    # preprocesses data to make the model's job easier
    
    n_samples = len(X)
    
    if min(X) < 0:          
        X = np.array(X) - [min(np.array(X))]*len(X)
    
    
    if max(Y) < 0:
        Y = [-Y[i] for i in range(0, len(Y))]
    
    if (Y[0] > Y[n_samples // 2] and Y[n_samples // 2] > Y[n_samples - 1]) or (Y[0] < Y[n_samples // 2] and Y[n_samples - 1] < Y[n_samples // 2]):
        Y = [-Y[i] for i in range(0, len(Y))]
        
    X = list(X)
    Y = list(Y)
    
    threshold = 400                                       # minimum data size (after data augmentation)
    
    X1 = X
    Y1 = Y
    
    if len(X) < threshold:
        while len(X) <= threshold and len(Y) <= threshold:
            X = X + X1
            Y = Y + Y1 
            
    # All the above three if cases manipulate the input data such that the model can find the degree more convenietly
    # We also make some use of somemakeshift data augmentation (replicating data points) to make it easier for the model.
    # Trust me! Replicating the data works pretty well in this case!
    
    X = np.array(X)
    Y = np.array(Y)
    
    Y = Y - np.mean(Y)
    
    X = np.reshape(X, (len(X), 1))
    Y = np.reshape(Y, (len(Y), 1))
    
    return X, Y
    
    
    
def buildModel():                                                  # builds polynomial net model with layers with exponential
                                                                   # output
    import keras
    import tensorflow as tf
    
    from keras.models import Model
    from keras.layers import Input, Dense, Lambda, multiply, add
    
    a = tf.Variable(1.0)
    b = tf.Variable(1.0)
    n = tf.Variable(1.0)
    
    x = Input(shape=(1,))
    
    j1 = 1.0
    j2 = 5.0
                                 # Hyperparameters that have been set by continuous trail and error
    k1 = 1.0
    k2 = 10.0
    
    dense_a = Dense(2)(x)
    powered_dense1 = Lambda(lambda x: tf.abs(-j1*tf.sigmoid(j2*a)*x**2 - k1*tf.sigmoid(k2*b))**(n/8.0))(Dense(1)(dense_a))
    model1 = Model(x, powered_dense1)
    model1.layers[-1].trainable_weights.extend([a, b, n])
    
    dense_b = Dense(2)(x)
    powered_dense2 = Lambda(lambda x: tf.abs(-j1*tf.sigmoid(j2*a)*x**2 + -k1*tf.sigmoid(k2*b))**(n/8.0))(Dense(1)(dense_b))
    model2 = Model(x, powered_dense2)
    model2.layers[-1].trainable_weights.extend([a, b, n])
    
    

                                                              # "n" is the parameter representing the degree
                                                     
                                                     
    dense_c = Dense(2)(x)
    powered_dense3 = Lambda(lambda x: tf.abs(j1*tf.sigmoid(j2*a)*x**2 + k1*tf.sigmoid(k2*b))**(n/8.0))(Dense(1)(dense_c))
    model3 = Model(x, powered_dense3)
    model3.layers[-1].trainable_weights.extend([a, b, n])

    dense_d = Dense(2)(x)
    powered_dense4 = Lambda(lambda x: tf.abs(j1*tf.sigmoid(j2*a)*x**2 + k1*tf.sigmoid(k2*b))**(n/8.0))(Dense(1)(dense_d))
    model4 = Model(x, powered_dense4)
    model4.layers[-1].trainable_weights.extend([a, b, n])
    
    c = tf.Variable([[0.0]])
    constantLayer = Lambda(lambda x: c)(x)
    
    exponentialLayers = multiply([model1.output, model2.output, model3.output, model4.output])
    
    model = Model(x, add([exponentialLayers, constantLayer]))
    model.layers[-1].trainable_weights.extend([a, b, c, n])
    
    return model, n
    
    
    
    
def trainModel(X, Y, model):                           # fits model to data
    model.fit(X, Y, epochs=1, verbose=0)
    
    
    
def predictDegree(X, Y):  
    import tensorflow as tf
    import numpy as np                                # Predicts degree of polynomial relationship between X and Y
    from math import isnan
    
    X, Y = preprocessData(X, Y)
    model, n = buildModel()
    
    from keras.optimizers import RMSprop
    model.compile(loss='mae', optimizer=RMSprop(lr=0.01))
    
    tf.global_variables_initializer().run()
    
    constant = 300000
    
    X_list = np.reshape(X, (len(X)))
    epochs = constant//(len(X_list))
    
    for i in range(0, epochs):
        trainModel(X, Y, model)             
        i = i + 1
        
    if sess.run(n) < 0:
        predictDegree(X, Y)
        
    return np.int32(np.round(sess.run(n)))    # return rounded degree      
    
 ***************************************************************************************************************************
           
                                             EXAMPLE OF USING THIS MODEL:
                                            
 Check out this polynomial regression dataset here: http://users.stat.ufl.edu/~winner/data/heatcapacity.dat and the data
 description here : http://users.stat.ufl.edu/~winner/data/heatcapacity.txt. I will use model to find the degree of the
 polynomial relationship between the two variables involved here: The heat capacities and temperatures of solid Hydrogen
 Bromide in 18 experimental runs. We take the heat capacity as the X variable and the temperature as the Y variable (the
 units of measure are given in the data description).
 
import time

startTime = time.time()
print("Degree of polynomial relationship : " + str(predictDegree(X, Y)) + " (if there is a polynomial relationship)")
endTime = time.time()
print("Time taken : " + str(endTime - startTime) + " s")
                           
# In the above code, we predict the degree and measure the time the whole process takes.

X = [10.79,  
     10.80,  
     10.86,  
     10.93,  
     10.99,  
     10.96,  
     10.98,
     11.03,  
     11.08,  
     11.10,  
     11.19,  
     11.25,  
     11.40,  
     11.61,  
     11.69,  
     11.91,  
     12.07,  
     12.32]

Y = [118.99,
     120.76,
     122.71,
     125.48,
     127.31,
     130.06,
     132.41,
     135.89,
     139.02,
     140.25,
     145.61,
     153.45,
     158.03,
     162.72,
     167.67,
     172.86,
     177.52,
     182.09]
     
import numpy as np
best_fit = np.polyfit(X, Y, predictDegree(X, Y))    # Coefficients of the best fitting polynomial of the "correct" degree

import matplotlib.pyplot as plt

plt.plot(X, Y, 'r.')

plt.plot(X, np.multiply(X, np.multiply(X, [best_fit[0]]*len(X))) + np.multiply(X, [best_fit[1]]*len(X)) + [best_fit[2]]*len(X), 'k')
# The predicted degree is 2 in this case and therefore the best_fit array has 3 elements for 3 coefficients

plt.show()

# Above, we plot and visualize the best fitting curve of the required degree alongside the actual data. The data does seem to 
  follow a quadratic curve. The model outputs the correct degree (which is 2). The whole degree prediction process takes 
  approximately 55.77 seconds on a 3.1 GHz Intel i5 core on an iMac.
